{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af93ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing matrix (x_{i,j}):\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "Task -> Agent -> Action mapping:\n",
      "Task 0 -> Agent 0 -> Action 0\n",
      "Task 1 -> Agent 1 -> Action 0\n",
      "Task 2 -> Agent 0 -> Action 0\n",
      "Task 3 -> Agent 2 -> Action 1\n",
      "Task 4 -> Agent 1 -> Action 0\n",
      "\n",
      "Success Rate: 0.68, Total Cost: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 환경 설정\n",
    "# -------------------------------\n",
    "\n",
    "num_tasks = 5      # 태스크 수\n",
    "num_agents = 3     # 에이전트 수\n",
    "num_actions = 2    # 각 에이전트가 선택할 수 있는 행동 수\n",
    "\n",
    "# 태스크 난이도 또는 요구 리소스\n",
    "task_difficulty = np.random.randint(1, 10, size=num_tasks)\n",
    "\n",
    "# 에이전트 리소스 (예: 처리 능력)\n",
    "agent_capacity = np.random.randint(5, 15, size=num_agents)\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 라우팅 레이어 (Routing)\n",
    "# -------------------------------\n",
    "\n",
    "def route_tasks(task_difficulty, agent_capacity):\n",
    "    \"\"\"\n",
    "    단순 라우팅 예시: \n",
    "    각 태스크를 처리 가능 용량이 충분한 에이전트에 무작위 할당\n",
    "    \"\"\"\n",
    "    routing = np.zeros((num_tasks, num_agents))  # x_{i,j}\n",
    "    \n",
    "    for i, difficulty in enumerate(task_difficulty):\n",
    "        possible_agents = [j for j, cap in enumerate(agent_capacity) if cap >= difficulty]\n",
    "        if possible_agents:\n",
    "            selected_agent = random.choice(possible_agents)\n",
    "            routing[i, selected_agent] = 1\n",
    "        else:\n",
    "            # 처리 불가능 시 가장 여유 있는 에이전트 선택\n",
    "            selected_agent = np.argmax(agent_capacity)\n",
    "            routing[i, selected_agent] = 1\n",
    "    return routing\n",
    "\n",
    "routing_matrix = route_tasks(task_difficulty, agent_capacity)\n",
    "print(\"Routing matrix (x_{i,j}):\")\n",
    "print(routing_matrix)\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 정책 레이어 (Decision / Policy)\n",
    "# -------------------------------\n",
    "\n",
    "def agent_policy(agent_id, task_id):\n",
    "    \"\"\"\n",
    "    단순 정책 예시:\n",
    "    - 행동 0: 빠른 처리, 낮은 정확도\n",
    "    - 행동 1: 느린 처리, 높은 정확도\n",
    "    \"\"\"\n",
    "    # 예: 태스크 난이도가 높으면 정확도 중심 행동 선택\n",
    "    if task_difficulty[task_id] > 5:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    return action\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Joint Routing + Policy 수행\n",
    "# -------------------------------\n",
    "\n",
    "results = []\n",
    "for i in range(num_tasks):\n",
    "    agent_id = np.argmax(routing_matrix[i])\n",
    "    action_id = agent_policy(agent_id, i)\n",
    "    results.append((i, agent_id, action_id))\n",
    "\n",
    "print(\"\\nTask -> Agent -> Action mapping:\")\n",
    "for r in results:\n",
    "    print(f\"Task {r[0]} -> Agent {r[1]} -> Action {r[2]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ 성능 평가 (예시)\n",
    "# -------------------------------\n",
    "\n",
    "def evaluate(results):\n",
    "    \"\"\"\n",
    "    간단 평가: 성공률 + 처리 비용\n",
    "    \"\"\"\n",
    "    success = 0\n",
    "    cost = 0\n",
    "    for task_id, agent_id, action_id in results:\n",
    "        # 성공 확률 예시\n",
    "        prob_success = 0.5 + 0.05 * agent_capacity[agent_id] - 0.05 * task_difficulty[task_id]\n",
    "        success += prob_success\n",
    "        \n",
    "        # 비용 예시: 행동 0 -> 낮음, 행동 1 -> 높음\n",
    "        cost += 1 if action_id == 0 else 2\n",
    "    return success/num_tasks, cost\n",
    "\n",
    "success_rate, total_cost = evaluate(results)\n",
    "print(f\"\\nSuccess Rate: {success_rate:.2f}, Total Cost: {total_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790b1e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/dibaeck/miniconda3/envs/agent/lib/python3.11/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/2ecfe594c29f43df90ab0ed5cf528189-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /tmp/2ecfe594c29f43df90ab0ed5cf528189-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 13 COLUMNS\n",
      "At line 88 RHS\n",
      "At line 97 BOUNDS\n",
      "At line 113 ENDATA\n",
      "Problem MODEL has 8 rows, 15 columns and 30 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 30.0833 - 0.00 seconds\n",
      "Cgl0003I 2 fixed, 0 tightened bounds, 1 strengthened rows, 8 substitutions\n",
      "Cgl0004I processed model has 4 rows, 7 columns (7 integer (7 of which binary)) and 12 elements\n",
      "Cutoff increment increased from 1e-05 to 0.9999\n",
      "Cbc0038I Initial state - 0 integers unsatisfied sum - 0\n",
      "Cbc0038I Solution found of -26\n",
      "Cbc0038I Before mini branch and bound, 7 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I After 0.00 seconds - Feasibility pump exiting with objective of -26 - took 0.00 seconds\n",
      "Cbc0012I Integer solution of -26 found by feasibility pump after 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0001I Search completed - best objective -26, took 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from -26 to -26\n",
      "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                26.00000000\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.00\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n",
      "Routing matrix (MIP optimized):\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Task -> Agent -> Action -> Reward mapping:\n",
      "Task 0 -> Agent 1 -> Action 0 -> Reward 1\n",
      "Task 1 -> Agent 0 -> Action 0 -> Reward 1\n",
      "Task 2 -> Agent 2 -> Action 0 -> Reward 1\n",
      "Task 3 -> Agent 2 -> Action 0 -> Reward 1\n",
      "Task 4 -> Agent 2 -> Action 0 -> Reward 1\n",
      "\n",
      "Total Reward (System Performance): 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pulp import LpMaximize, LpProblem, LpVariable, lpSum\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 환경 설정\n",
    "# -------------------------------\n",
    "num_tasks = 5\n",
    "num_agents = 3\n",
    "num_actions = 2\n",
    "\n",
    "# 태스크 난이도\n",
    "task_difficulty = np.random.randint(1, 10, size=num_tasks)\n",
    "# 에이전트 처리 능력\n",
    "agent_capacity = np.random.randint(5, 15, size=num_agents)\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 라우팅 레이어 (MIP 최적화)\n",
    "# -------------------------------\n",
    "# 목표: 처리 효율 + 에이전트 여유 자원 최대화\n",
    "\n",
    "model = LpProblem(name=\"routing_optimization\", sense=LpMaximize)\n",
    "\n",
    "# 변수: x[i,j] = 1 if task i is assigned to agent j\n",
    "x = [[LpVariable(f\"x_{i}_{j}\", cat=\"Binary\") for j in range(num_agents)] for i in range(num_tasks)]\n",
    "\n",
    "# 목표 함수 예시\n",
    "model += lpSum(\n",
    "    x[i][j] * (agent_capacity[j] - task_difficulty[i])  # 단순 효율 평가\n",
    "    for i in range(num_tasks)\n",
    "    for j in range(num_agents)\n",
    ")\n",
    "\n",
    "# 제약 조건: 각 태스크는 하나의 에이전트에만 할당\n",
    "for i in range(num_tasks):\n",
    "    model += lpSum(x[i][j] for j in range(num_agents)) == 1\n",
    "\n",
    "# 제약 조건: 에이전트 용량 초과 불가\n",
    "for j in range(num_agents):\n",
    "    model += lpSum(x[i][j]*task_difficulty[i] for i in range(num_tasks)) <= agent_capacity[j]\n",
    "\n",
    "model.solve()\n",
    "\n",
    "routing_matrix = np.array([[x[i][j].varValue for j in range(num_agents)] for i in range(num_tasks)])\n",
    "print(\"Routing matrix (MIP optimized):\")\n",
    "print(routing_matrix)\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 정책 레이어 (RL 기반 단순화)\n",
    "# -------------------------------\n",
    "# Q-learning 스타일 간단 정책\n",
    "# 상태: 태스크 난이도, agent capacity\n",
    "# 행동: 0=빠른 처리/낮은 정확도, 1=느린 처리/높은 정확도\n",
    "\n",
    "Q_table = np.zeros((num_tasks, num_actions))  # 상태별 행동 가치\n",
    "\n",
    "def rl_policy(task_id, epsilon=0.1):\n",
    "    # 탐험 vs 활용\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0,1])\n",
    "    else:\n",
    "        return np.argmax(Q_table[task_id])\n",
    "\n",
    "def update_q(task_id, action, reward, alpha=0.5):\n",
    "    # 단순 Q 업데이트\n",
    "    Q_table[task_id, action] = (1-alpha)*Q_table[task_id, action] + alpha*reward\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Joint Routing + RL Policy 수행\n",
    "# -------------------------------\n",
    "results = []\n",
    "for i in range(num_tasks):\n",
    "    agent_id = np.argmax(routing_matrix[i])\n",
    "    action_id = rl_policy(i)\n",
    "    \n",
    "    # 간단 보상 예시: 난이도가 높으면 정확도 중심 행동(1) 보상이 높음\n",
    "    reward = 1 + 0.5*task_difficulty[i] if action_id==1 else 1\n",
    "    update_q(i, action_id, reward)\n",
    "    \n",
    "    results.append((i, agent_id, action_id, reward))\n",
    "\n",
    "print(\"\\nTask -> Agent -> Action -> Reward mapping:\")\n",
    "for r in results:\n",
    "    print(f\"Task {r[0]} -> Agent {r[1]} -> Action {r[2]} -> Reward {r[3]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ 성능 평가\n",
    "# -------------------------------\n",
    "total_reward = sum(r[3] for r in results)\n",
    "print(f\"\\nTotal Reward (System Performance): {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a90f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/dibaeck/miniconda3/envs/agent/lib/python3.11/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/c73886353adc4bbf9d9a010a8cf6dbe3-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /tmp/c73886353adc4bbf9d9a010a8cf6dbe3-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 13 COLUMNS\n",
      "At line 87 RHS\n",
      "At line 96 BOUNDS\n",
      "At line 112 ENDATA\n",
      "Problem MODEL has 8 rows, 15 columns and 30 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 46.6 - 0.00 seconds\n",
      "Cgl0004I processed model has 7 rows, 13 columns (13 integer (13 of which binary)) and 26 elements\n",
      "Cutoff increment increased from 1e-05 to 0.9999\n",
      "Cbc0038I Initial state - 2 integers unsatisfied sum - 0.4\n",
      "Cbc0038I Pass   1: suminf.    0.40000 (2) obj. -45.2 iterations 3\n",
      "Cbc0038I Solution found of -38\n",
      "Cbc0038I Before mini branch and bound, 9 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 7 rows 13 columns, reduced to 0 rows 0 columns\n",
      "Cbc0038I Mini branch and bound improved solution from -38 to -45 (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of -46.0599\n",
      "Cbc0038I Reduced cost fixing fixed 6 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.40000 (2) obj. -46.6 iterations 1\n",
      "Cbc0038I Pass   3: suminf.    0.47005 (1) obj. -46.0599 iterations 4\n",
      "Cbc0038I Pass   4: suminf.    0.33333 (1) obj. -46.3333 iterations 1\n",
      "Cbc0038I Pass   5: suminf.    0.33333 (1) obj. -46.3333 iterations 0\n",
      "Cbc0038I Pass   6: suminf.    0.33333 (1) obj. -46.3333 iterations 0\n",
      "Cbc0038I Pass   7: suminf.    0.33333 (1) obj. -46.3333 iterations 0\n",
      "Cbc0038I Pass   8: suminf.    0.33333 (1) obj. -46.3333 iterations 0\n",
      "Cbc0038I Pass   9: suminf.    0.80000 (2) obj. -46.2 iterations 1\n",
      "Cbc0038I Pass  10: suminf.    0.94009 (2) obj. -46.0599 iterations 2\n",
      "Cbc0038I Pass  11: suminf.    0.33333 (1) obj. -46.3333 iterations 2\n",
      "Cbc0038I Pass  12: suminf.    0.47005 (1) obj. -46.0599 iterations 1\n",
      "Cbc0038I Pass  13: suminf.    0.40000 (2) obj. -46.6 iterations 2\n",
      "Cbc0038I Pass  14: suminf.    0.40000 (2) obj. -46.6 iterations 1\n",
      "Cbc0038I Pass  15: suminf.    0.47004 (1) obj. -46.0599 iterations 2\n",
      "Cbc0038I Pass  16: suminf.    0.33333 (1) obj. -46.3333 iterations 1\n",
      "Cbc0038I Pass  17: suminf.    0.94009 (2) obj. -46.0599 iterations 3\n",
      "Cbc0038I Pass  18: suminf.    0.33333 (1) obj. -46.3333 iterations 2\n",
      "Cbc0038I Pass  19: suminf.    0.47005 (1) obj. -46.0599 iterations 1\n",
      "Cbc0038I Pass  20: suminf.    0.47005 (1) obj. -46.0599 iterations 0\n",
      "Cbc0038I Pass  21: suminf.    0.33333 (1) obj. -46.3333 iterations 1\n",
      "Cbc0038I Pass  22: suminf.    0.47005 (1) obj. -46.0599 iterations 1\n",
      "Cbc0038I Pass  23: suminf.    0.47005 (1) obj. -46.0599 iterations 0\n",
      "Cbc0038I Pass  24: suminf.    0.33333 (1) obj. -46.3333 iterations 1\n",
      "Cbc0038I Pass  25: suminf.    0.33333 (1) obj. -46.3333 iterations 0\n",
      "Cbc0038I Pass  26: suminf.    0.40000 (2) obj. -46.6 iterations 3\n",
      "Cbc0038I Pass  27: suminf.    0.40000 (2) obj. -46.6 iterations 1\n",
      "Cbc0038I Pass  28: suminf.    0.47004 (1) obj. -46.0599 iterations 2\n",
      "Cbc0038I Pass  29: suminf.    0.33333 (1) obj. -46.3333 iterations 1\n",
      "Cbc0038I Pass  30: suminf.    0.94009 (2) obj. -46.0599 iterations 3\n",
      "Cbc0038I Pass  31: suminf.    0.33333 (1) obj. -46.3333 iterations 2\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 8 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I After 0.00 seconds - Feasibility pump exiting with objective of -45 - took 0.00 seconds\n",
      "Cbc0012I Integer solution of -45 found by feasibility pump after 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from -46.6 to -46.6 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective -45, took 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0035I Maximum depth 0, 6 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from -46.6 to -46.6\n",
      "Probing was tried 1 times and created 1 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                45.00000000\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.00\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n",
      "Routing matrix (MIP optimized):\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Task -> Agent -> Action -> Reward mapping:\n",
      "Task 0 -> Agent 2 -> Action 0 -> Reward 1\n",
      "Task 1 -> Agent 0 -> Action 0 -> Reward 1\n",
      "Task 2 -> Agent 2 -> Action 0 -> Reward 1\n",
      "Task 3 -> Agent 2 -> Action 0 -> Reward 1\n",
      "Task 4 -> Agent 0 -> Action 0 -> Reward 1\n",
      "\n",
      "Total Reward (System Performance): 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pulp import LpMaximize, LpProblem, LpVariable, lpSum\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 환경 설정\n",
    "# -------------------------------\n",
    "num_tasks = 5\n",
    "num_agents = 3\n",
    "num_actions = 2\n",
    "\n",
    "task_difficulty = np.random.randint(1, 10, size=num_tasks)\n",
    "agent_capacity = np.random.randint(5, 15, size=num_agents)\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 라우팅 레이어 (MIP)\n",
    "# -------------------------------\n",
    "def routing_mip(tasks, agents):\n",
    "    model = LpProblem(name=\"routing_optimization\", sense=LpMaximize)\n",
    "    x = [[LpVariable(f\"x_{i}_{j}\", cat=\"Binary\") for j in range(num_agents)] for i in range(num_tasks)]\n",
    "    \n",
    "    # 목표: 에이전트 여유 자원 + 효율\n",
    "    model += lpSum(x[i][j] * (agents[j] - tasks[i]) for i in range(num_tasks) for j in range(num_agents))\n",
    "    \n",
    "    # 제약: 각 태스크 하나의 에이전트에만 할당\n",
    "    for i in range(num_tasks):\n",
    "        model += lpSum(x[i][j] for j in range(num_agents)) == 1\n",
    "    # 제약: 에이전트 용량 초과 금지\n",
    "    for j in range(num_agents):\n",
    "        model += lpSum(x[i][j]*tasks[i] for i in range(num_tasks)) <= agents[j]\n",
    "    \n",
    "    model.solve()\n",
    "    return np.array([[x[i][j].varValue for j in range(num_agents)] for i in range(num_tasks)])\n",
    "\n",
    "routing_matrix = routing_mip(task_difficulty, agent_capacity)\n",
    "print(\"Routing matrix (MIP optimized):\")\n",
    "print(routing_matrix)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ 정책 레이어 (RL 기반 단순화)\n",
    "# -------------------------------\n",
    "Q_table = np.zeros((num_tasks, num_actions))\n",
    "\n",
    "def rl_policy(task_id, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0,1])\n",
    "    return np.argmax(Q_table[task_id])\n",
    "\n",
    "def update_q(task_id, action, reward, alpha=0.5):\n",
    "    Q_table[task_id, action] = (1-alpha)*Q_table[task_id, action] + alpha*reward\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Joint 실행\n",
    "# -------------------------------\n",
    "results = []\n",
    "for i in range(num_tasks):\n",
    "    agent_id = np.argmax(routing_matrix[i])\n",
    "    action_id = rl_policy(i)\n",
    "    reward = 1 + 0.5*task_difficulty[i] if action_id==1 else 1\n",
    "    update_q(i, action_id, reward)\n",
    "    results.append((i, agent_id, action_id, reward))\n",
    "\n",
    "print(\"\\nTask -> Agent -> Action -> Reward mapping:\")\n",
    "for r in results:\n",
    "    print(f\"Task {r[0]} -> Agent {r[1]} -> Action {r[2]} -> Reward {r[3]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ 성능 평가\n",
    "# -------------------------------\n",
    "total_reward = sum(r[3] for r in results)\n",
    "print(f\"\\nTotal Reward (System Performance): {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
